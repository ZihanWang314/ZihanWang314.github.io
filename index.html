<!DOCTYPE HTML>
<html lang="en"><head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-1Y9L74WH97"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
  
      gtag('config', 'G-1Y9L74WH97');
    </script>
  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zihan Wang's Homepage 王子涵</title>
  
  <meta name="author" content="Zihan Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
	
	<style>
	  /* Style for the floating window */
	  #floatWindow {
	      display: none; /* Hidden by default */
	      position: absolute;
	      background-color: #f9f9f9;
	      border: 1px solid #ccc;
	      padding: 15px;
	      width: 300px;
	      box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.2);
	      z-index: 100;
	      top: 100px;
	      left: 100px;
	  }
	
	  /* Close button style */
	  #closeBtn {
	      display: inline-block;
	      float: right;
	      cursor: pointer;
	      color: #aaa;
	  }
	
	  #closeBtn:hover {
	      color: #000;
	  }
  table {
    width: 100%;
    border-collapse: collapse;
  }

  td, th {
    padding: 8px;
  }
	</style>

<style>
  /* 当屏幕宽度小于1200px时，调整字体大小 */
  @media (max-width: 1200px) {
    #responsiveTable td,
    #responsiveTable td * {
      font-size: 14px;
    }
      /* 调整图片大小 */
    #resizeImg {
      height: 16px; /* 图片高度等于字体大小 */
      width: auto; /* 按比例调整宽度 */
      vertical-align: middle; /* 保持图片与文字对齐 */
    }
  }

  /* 当屏幕宽度小于800px时，进一步调整字体大小 */
  @media (max-width: 800px) {
    #responsiveTable td,
    #responsiveTable td * {
      font-size: 12px;
    }
    #resizeImg {
      height: 14px; /* 图片高度等于字体大小 */
      width: auto; /* 按比例调整宽度 */
      vertical-align: middle; /* 保持图片与文字对齐 */
    }
  }

  /* 当屏幕宽度小于500px时，字体更小 */
  @media (max-width: 500px) {
    #responsiveTable td,
    #responsiveTable td * {
      font-size: 10px;
    }
    #resizeImg {
      height: 12px; /* 图片高度等于字体大小 */
      width: auto; /* 按比例调整宽度 */
      vertical-align: middle; /* 保持图片与文字对齐 */
    }
  }
</style>


</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Zihan Wang</name>
              </p>
              <p>
                I am a first-year CS PhD student at 
		<a href="https://www.northwestern.edu/">Northwestern University</a>,
                advised by the wonderful
                <a href="https://limanling.github.io">Manling Li</a>.
                I got my bachelor's degree with the Baosteel Award at 
                <a href="https://ai.ruc.edu.cn">Gaoling School of AI, RUC</a>.
		I was fortunate to work with  
		<a href="http://blender.cs.illinois.edu">Heng Ji</a>
		at UIUC and collaborate with fantastic teams at
                <a href="https://www.deepseek.com/">DeepSeek</a>.
		<br>
		My Chinese name is 王子涵. You can pronounce my name as "Tsz-han Wang". 
              </p>
              <p style="text-align:center">
                <!-- <a style="color:red" href="https://www.overleaf.com/read/dgwkwnknkprx#1bbeb2">Research Statement</a> -->
                <!-- &nbsp<br>&nbsp -->
                <a href="mailto:510642032wzh@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://github.com/zihanwang314">Github</a> &nbsp/&nbsp
                <a href="https://www.semanticscholar.org/author/Zihan-Wang/2243360876">Semantic Scholar</a> &nbsp/&nbsp
                <a href="https://www.zhihu.com/people/platinum-47-88">Zhihu</a>  &nbsp/&nbsp
                <a href="pdf/ZihanWang_CV.pdf">CV</a>
<!-- 		, <a href="https://www.overleaf.com/read/mxyjqgqvvrty#c2adfb">overleaf(up-to-date)</a>]  &nbsp/&nbsp -->
		<br>
		<a href="https://twitter.com/wzihanw?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @wzihanw</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/photo.jpeg"><img
                      style="width:80%;max-width:80%" alt="profile photo" src="images/1011_profile.JPG" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>









        
        <heading>News</heading>
	<table width="100%" align="center" border="0" cellpadding="20">
	  <tbody>
	    <tr>
	      <td style="padding:20px;width:75%;vertical-align:middle">
	        <ul>
	          <!-- <li>
	            <b>🗓️ Oct 29, 2024</b> - 
			Releasing <a href="http://www.github.com/zihanwang314/dump-to-gpt"> dump-to-GPT</a>:
			let GPT quickly read your entire codebase with one line of code! 
			A small but exciting start of <a href="http://www.github.com/zihanwang314/AI-wrench"> AI-wrench</a>,
			a growing toolkit for efficient AI engineering.
		  </li> -->
	          <li>
	            <b>🗓️ Sep 20, 2024</b> - Glad to announce that
	            <a href="https://arxiv.org/abs/2407.01906">ESFT</a>
	            has been accepted to the EMNLP 2024 Main Conference! 🎉 Many thanks to all collaborators!
	          </li>
	          <li>
	            <b>🗓️ Jul 4, 2024</b> - 
			Thrilled to introduce our latest project at DeepSeek,
	            <a href="https://arxiv.org/abs/2407.01906">Expert-Specialized Fine-Tuning (ESFT)</a>
		    for efficient and effective LLM customization by leveraging the highly specialized Mixture-of-Experts (MoE)
		    architecture! 🤖✨
	          </li>
	          <li>
	            <b>🗓️ Jun 2, 2024</b> - Grateful to be spotlighted by my alma mater RUC for my journey and achievements. (<a href="https://mp.weixin.qq.com/s?__biz=MzA3ODU4MzU3OA==&mid=2652442865&idx=1&sn=5ac4b8798062e581cee358e89958bc76">read blog</a>)
	          </li>			
	          <li><b>🗓️ Feb 15, 2024</b> - Excited to join Northwestern as a PhD student! 🎓 Many thanks to my advisor <a href="https://limanling.github.io">Manling Li</a>!
	          </li>
	          <li><b>🗓️ Oct 19, 2023</b> - Honored to be awarded the 
	            <a href="https://mp.weixin.qq.com/s/Nb6RIBYcZQp_K66wvEr34A">Baosteel Outstanding Student Award 2023</a> 🏅 as the <b>ONLY</b> undergrad student among science and technology departments in RUC! Special thanks to 
	            <a href="http://playbigdata.ruc.edu.cn/dou">NLPIR lab</a>! 🙏
	          </li>
	          <li><b>🗓️ Jun 7, 2023</b> - Excited to share that I'll be joining 
	            <a href="https://blender.cs.illinois.edu/">UIUC Blender Lab</a> 🔬 this summer as a student researcher!
	          </li>
	          <!-- <li><b>🗓️ Mar 15, 2023</b> - My talk on <font size="3">LARGE</font> language models at 
	            <a href="https://cosx.org/">Capital of Statistics</a> 📊 will take place at 7:00 PM Mar 17, 2023 BJT! Click
	            <a href="https://mp.weixin.qq.com/s/9G8wct4ktTHqQw7FFFhbQg">here</a> for more details. (Update: <a href="./data/LLMs.pptx" download>slides</a>, <a href="https://www.bilibili.com/video/BV1EL411C79J">video</a>)
	          </li>
	          <li><b>🗓️ Jan 12, 2023</b> - I will give a talk on pre-trained models and their applications 📚 at 2:00 PM Jan 13, 2023 BJT at 
	            <a href="http://mlc.ruc.edu.cn/">Mingli College</a>! For more information, click
	            <a href="https://mp.weixin.qq.com/s/ecaswp2h04tCgL7O4B5RGQ">here</a>.
	            (Update: <a href="./data/pretraining.pptx" download>slides</a>)
	          </li> -->
	          <li><b>🗓️ Dec 12, 2022</b> - I posted an article introducing ChatGPT on 
	            <a href="https://cosx.org/">Capital of Statistics</a> 💡. Do not miss it if you want to know more about ChatGPT! (<a href="https://mp.weixin.qq.com/s/JiWpTORpjqzOgC8IsslKVA">link</a>)
	          </li>
	        </ul>
	      </td>
	    </tr>
	  </tbody>
	</table>
	      
        <heading>Research Interest</heading>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <p>The growth of foundation models, while extremely rapid, has heightened the need to address the challenges arising from their expanding scale.
                  My research focuses on foundation models' <b>autonomy</b> (<a href="https://openreview.net/forum?id=jp3gWrMuIZ">MINT benchmark</a>), <b>efficiency</b> (<a href="https://arxiv.org/abs/2405.04434">DeepSeek-V2</a>, <a href="https://arxiv.org/abs/2407.01906">Expert-Specialized Tuning</a>), and <b>long-context understanding</b> (<a href="https://doi.org/10.1145/3583780.3614993">NOVO</a>, <a href="https://arxiv.org/abs/2306.05212">RETA-LLM Toolkit</a>).
              </p>
            </td>
          </tr>
        </tbody></table>

        





	<heading>Selected Publications</heading>
	<p>See full list on <a href="https://www.semanticscholar.org/author/Zihan-Wang/2243360876" target="_blank">Semantic Scholar</a> (<a href="#" id="openFloat">Why I Love Semantic Scholar, and You Might Too</a>)</p> 
	<!-- The floating window -->
	<div id="floatWindow">
	    <span id="closeBtn">&times;</span>
	    <p>
        Semantic Scholar uses <b>AI-powered</b> tools to summarize papers, highlight key phrases, and rank research by influence.
        This helps you <b>find important studies faster</b>. 
        Its <b> Semantic Reader </b> helps you understand papers with <b> skimming highlights </b> and <b> citation cards </b>.
        You can also see how papers connect with citation graphs. 
        While Google Scholar is great for broad searches, 
        Semantic Scholar is smarter for finding high-quality and impactful research!
	    </p>
	</div>
	      

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;" id="responsiveTable">
          <tbody>

            <tr>
              <td style="padding:20px;width:27%;vertical-align:middle">
                <div class="one">
                  <img src='images/esft.png' width="190">
                </div>
              </td>
              <td style="padding:20px;width:67%;vertical-align:middle">
		<papertitle><span style="color:red;">[New]</span> Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models</papertitle>
                <br>
                <strong>Zihan Wang</strong>,
                <a href="https://victorchen96.github.io/chendeli.io/">Deli Chen</a>,
                <a href="https://scholar.google.com.hk/citations?user=8b-ysf0NWVoC&hl=zh-CN">Damai Dai</a>,
                <a href="https://runxinxu.github.io/aboutme/">Runxin Xu</a>,
                <a href="http://www.idi.zju.edu.cn/member/3053.html">Zhuoshu Li</a>,
                <a href="https://scholar.google.com/citations?user=aQizmzsAAAAJ&hl=zh-CN"> Yu Wu </a>
                <br>
                <em>EMNLP 2024</em>
                <br>
		<a href="https://arxiv.org/abs/2407.01906" style="vertical-align: middle;">[paper]</a>
		<a href="https://github.com/deepseek-ai/ESFT" style="vertical-align: middle;">[code]</a>
		<img id="resizeImg" alt="" src="https://img.shields.io/github/stars/deepseek-ai/ESFT?style=social" style="vertical-align: middle;">
                <p>
                  We harness the <b> Specialized Power of Experts </b> in MoE LLMs through ESFT. By fine-tuning <b> Down to 5% Experts </b> in a layer, near-full performance can be achieved. 
                </p>
                <br>
                </td>
            </tr>

            
          <tr>
            <td style="padding:20px;width:27%;vertical-align:middle">
              <div class="one">
                <img src='images/MINT.png' width="190">
              </div>
            </td>
            <td style="padding:20px;width:67%;vertical-align:middle">
              <papertitle><span style="color:red;">[Highlight]</span> MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback</papertitle>
              <br>
              <a href="https://xingyaoww.github.io">Xingyao Wang*</a>,
              <strong>Zihan Wang*</strong>,
              <a href="https://lumos-jiateng.github.io">Jiateng Liu</a>,
              <a href="https://yangyi-chen.github.io">Yangyi Chen</a>,
              <a href="https://lifan-yuan.github.io">Lifan Yuan</a>,
              <a href="https://haopeng-nlp.github.io/">Hao Peng</a>,
              <a href="http://blender.cs.illinois.edu/hengji.html">Heng Ji</a>
              <br>
              <em>ICLR 2024</em>
              <br>
		<a href="https://arxiv.org/abs/2309.10691" style="vertical-align: middle;">[paper]</a>
		<a href="https://xingyaoww.github.io/mint-bench" style="vertical-align: middle;">[website]</a>
		<a href="https://github.com/xingyaoww/mint-bench" style="vertical-align: middle;">[code]</a>
		<img id="resizeImg" alt="" src="https://img.shields.io/github/stars/xingyaoww/mint-bench?style=social" style="vertical-align: middle;">
              <p>
		We introduce <b>MINT</b>, a benchmark for evaluating LLMs in <b>Multi-turn Interactions</b> with tools and language feedback. 
                MINT reveals several limitations in existing RLHF and SIFT methods on multi-turn interaction.
              </p>
              <br>
              </td>
          </tr>

	  <tr>
              <td style="padding:20px;width:27%;vertical-align:middle">
                <div class="one">
                  <img src='https://github.com/deepseek-ai/DeepSeek-V2/raw/main/figures/activationparameters.png?raw=true' width="190">
                </div>
              </td>
              <td style="padding:20px;width:67%;vertical-align:middle">
		<papertitle><span style="color:red;">[Highlight]</span> DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model</papertitle>
                <br>
		<a href="https://chat.deepseek.com"> DeepSeek AI</a> (157 authors including <strong> Zihan Wang</strong>)
                <br>
		<a href="https://arxiv.org/abs/2405.04434" style="vertical-align: middle;">[paper]</a>
		<a href="https://github.com/deepseek-ai/DeepSeek-V2?tab=readme-ov-file" style="vertical-align: middle;">[code]</a>
		<img id="resizeImg" alt="" src="https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2?style=social" style="vertical-align: middle;">
                <p>
		DeepSeek-V2 is a strong MoE model with 23B activated parameters. It achieves stronger 
		performance compared to DeepSeek 67B, <b>saving 42.5% training costs and boosting generation by up to 5.76x</b>.
                </p>
                <br>
                </td>
            </tr>

          <tr>
            <td style="padding:20px;width:27%;vertical-align:middle">
              <div class="one">
                <img src='images/NOVO.png' width="190">
              </div>
            </td>
            <td style="padding:20px;width:67%;vertical-align:middle">
              <papertitle>NOVO: Learnable and Interpretable Document Identifiers for Model Based IR</papertitle>
              <br>
              <strong>Zihan Wang</strong>,
              <a href="https://www.zhouyujia.cn/">Yujia Zhou</a>,
              Yiteng Tu, 
              <a href="http://playbigdata.ruc.edu.cn/dou">Zhicheng Dou</a>.
              <br>
              <em>CIKM 2023, <b>Oral Presentation</b></em>
              <br>
		<a href="pdf/cikm23.pdf" style="vertical-align: middle;">[paper]</a>
		<a href="https://github.com/ZihanWang314/NOVO" style="vertical-align: middle;">[code]</a>
		<img id="resizeImg" alt="" src="https://img.shields.io/github/stars/zihanwang314/NOVO?style=social" style="vertical-align: middle;">
              <p> 
                We propose learnable NOVO document-IDs for model-based IR. 
                NOVO IDs consist of non-overlapping n-gram sets to identify documents,
                optimized through denoising queries and retrieval tasks.
              </p>
              <br>
              </td>
          </tr>

          
          <tr>
            <td style="padding:20px;width:27%;vertical-align:middle">
              <div class="one">
                <img src='images/retallm.png' width="190">
              </div>
            </td>
            <td style="padding:20px;width:67%;vertical-align:middle">
              <papertitle>RetaLLM: A Retrieval-Augmented Large Language Model Toolkit</papertitle>
              <br>
              <a href="https://github.com/rucliujn">Jiongnan Liu</a>,
              <a href="https://github.com/ignorejjj">Jiajie Jin</a>,
              <strong>Zihan Wang</strong>, 
              <a href="https://paperswithcode.com/search?q=author%3AJiehan+Cheng">Jiehan Cheng</a>,
              <a href="http://playbigdata.ruc.edu.cn/dou">Zhicheng Dou</a>,
              <a href="https://scholar.google.com/citations?hl=en&user=tbxCHJgAAAAJ">Ji-Rong Wen</a>
              <br>
		<a href="https://arxiv.org/abs/2306.05212" style="vertical-align: middle;">[paper]</a>
		<a href="https://github.com/ruc-gsai/yulan-ir" style="vertical-align: middle;">[code]</a>
		<img id="resizeImg" alt="" src="https://img.shields.io/github/stars/ruc-gsai/yulan-ir?style=social" style="vertical-align: middle;">
		
              <p> 
                We develop a Retreival-Augmented LLM toolkit for better interaction between LLMs and retrieval systems.
                Feature modules: request rewriting, passage extraction, and fact-checking.      
              </p>
              </td>
          </tr>

        </tbody></table>


        <heading>Awards</heading>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <ul>
		  <li><b>McCormick School of Engineering Fellowship</b>, Northwestern, 2024</li>
                  <li><b>Baosteel Outstanding Student Award</b>, 7/30000+, Renmin Univ. of China, 2023</li>
                  <li><b>First Class Academic Excellence Award</b> (top 3% GPA), Renmin Univ. of China, 2021</li>
                  <li><b>Provincal First Prize</b>, Contemporary Undergraduate Mathematical Contest in
                    Modeling, 2021</li>
                  <li><b>Honorable Mention</b>, Mathematical Contest in Modeling and Interdisciplinary
                    Contest in Modeling, 2021</li>
                </ul>
            </td>
          </tr>
        </tbody></table>

        
        <heading>Invited Talks and Presentations</heading>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
	    	<ul>
		  <li>
		    <a href="https://china-r.cosx.org/bj2023/lectures.html">LLM Agents with Language Feedback</a>, 
		    Chinese R Conference &nbsp;&nbsp;&nbsp;&nbsp; 2023.11
		  </li>
		  <li>
		    <a href="http://ai.ruc.edu.cn/newslist/newsdetail/20230606002.html">Retrieval Augmented Language Models and Applications</a>,
		    RUC Science and Technology Fair &nbsp;&nbsp;&nbsp;&nbsp; 2023.05
		  </li>
		  <li>
		    <a href="https://www.bilibili.com/video/BV1EL411C79J/?spm_id_from=333.337.search-card.all.click&vd_source=ad803afa85e584a3fc4cbfa14cca9a57">Large Language Models and Applications</a>,
		    Capital of Statistics &nbsp;&nbsp;&nbsp;&nbsp; 2023.03
		  </li>
		  <li>
		    <a href="https://mp.weixin.qq.com/s/ecaswp2h04tCgL7O4B5RGQ">Pre-trained Language Models and Applications</a>,
		    RUC Mingli College &nbsp;&nbsp;&nbsp;&nbsp; 2023.01
		  </li>
		</ul>
            </td>
          </tr>
        </tbody></table>


        <heading>Professional Service</heading>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
	    	<ul>
			  <li>
				  <b>Reviewer - </b> 
				  <a href="https://2024.emnlp.org/calls/demos/">EMNLP 2024</a> (<a href="https://x.com/emnlpmeeting/status/1857169065569292540">Outstanding Reviewer</a>), 
				  <a href="https://iclr.cc/">ICLR 2025</a> (external)
			  </li>
			  <li><b>Session Organization - </b> 
				  Session of Language Models & Agents in <a href="https://china-r.cosx.org/bj2023/lectures.html">Chinese R Conference 2023</a>,
				   BoF and Affinity Group at <a href="https://2023.emnlp.org/program/bof/">EMNLP 2024</a>.
			  </li>
			  <li><b>Academic Mentor - </b> 
				  <a href="http://ai.ruc.edu.cn/newslist/notice/20230411001.html">National University Student Innovation Program (2023, No. 3)</a>
			  </li>
			<li><b>Article Translation: English - </b> 
				<a href="https://drive.google.com/file/d/1DW5ohZWxoCEOdrUQjokKreuArHqJdtKb/view">Unveiling DeepSeek: A Story of Even More Radical Chinese Technological Idealism</a>, 
				<b>Chinese - </b> 
				<a href="https://cosx.org/2021/11/interview-of-rubin/">COS Interview with Donald B. Rubin</a>, 
				<a href="https://mp.weixin.qq.com/s/UL0BK3s2CXVXUivhzU5ZKw">Core Views on AI Safety: When, Why, What, and How</a>
			</li>

                </ul>
            </td>
          </tr>
        </tbody></table>


        







        <heading>Misc</heading>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
                <ul>
                    <li> I like to work and chat with people from diverse backgrounds (🌈), 
			    which I believe is the key to true innovation. 
			    Feel free to reach out for an online chat (or in person if you are in Evanston / Chicago Area). </li>
                  <li>I write articles on Zhihu, a popular Q&A platform in China. Here are a few selected posts that I believe others have found valuable: <br>
                - <a href="https://www.zhihu.com/question/305278806/answer/3130954977">How do you determine if a professor is an academic rising star?</a> <br>
                - <a href="https://www.zhihu.com/question/424422013/answer/3236350492">From a non-elite university, winning many awards in ACM competitions. However, my low GPA keeps me away from scholarships—am I useless?</a> <br>
                - <a href="https://www.zhihu.com/question/589704718/answer/2955855409">How has your NLP research changed since GPT-4's release?</a>
                  </li>
                  <li>I love Sandbox games like Minecraft, Danmaku games like Touhou Project, and Music games like Love Live.
                    I also loved to design and make RPG games when I was in primary school (with 
                    <a href="https://www.rpgmakerweb.com/products/rpg-maker-xp">RMXP</a> on WindowsXP).
                  </li>
                  <li>My dream was to be a vlogger and I post 
                    <a href="https://space.bilibili.com/2066036?spm_id_from=333.337.0.0">videos</a>
                    on bilibili, including vlogs, game playing records and some parody videos. 
                  </li>
                  <li>Beyond Chinese and English, I’ve picked up some Japanese due to my childhood love for anime. 
    My favorite Anime was ワンピース and Fate/stay night.
                  </li>
                </ul>
            </td>
          </tr>


        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Website design from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
    <tr style="padding:0px">
      <td style="padding:2.5%;width:40%;vertical-align:middle">
        <div class="page__footer">
          <footer> <!-- start custom footer snippets --> 
            <a href="/sitemap/">Sitemap</a> 
            <!-- end custom footer snippets -->
            <div class="page__footer-copyright">
              <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=o89zY8a0ARtuHIK-X34murs3PrRBFGrB1jTX7j2kYt8&cl=ffffff&w=a"></script>
            </div>
          </footer>
        </div>
      </td>
    </tr>
  </tbody></table>

  <script>
    // Get the elements
    const floatWindow = document.getElementById('floatWindow');
    const openFloat = document.getElementById('openFloat');
    const closeBtn = document.getElementById('closeBtn');
  
    // Open the float window when the link is clicked
    openFloat.addEventListener('click', function(event) {
        event.preventDefault(); // Prevent the default action of the link
  
        // Get the position of the link element
        const rect = openFloat.getBoundingClientRect();
  
        // Position the floating window near the link
        floatWindow.style.top = rect.top + window.scrollY + 30 + 'px'; // 30px below the link
        floatWindow.style.left = rect.left + window.scrollX + 'px'; // Align it with the link horizontally
  
        // Display the floating window
        floatWindow.style.display = 'block';
    });
  
    // Close the float window when the close button is clicked
    closeBtn.addEventListener('click', function() {
        floatWindow.style.display = 'none';
    });
  
    // Close the float window when clicking outside of it
    window.addEventListener('click', function(event) {
        if (event.target !== floatWindow && event.target !== openFloat && !floatWindow.contains(event.target)) {
            floatWindow.style.display = 'none';
        }
    });
  </script>
  

  
</body>

</html>
